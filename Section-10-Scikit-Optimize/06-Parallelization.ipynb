{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization with Scikit-Optimize\n",
    "\n",
    "In this notebook, we will perform **Bayesian Optimization** with Gaussian Processes in Parallel, utilizing various CPUs, to speed up the search.\n",
    "\n",
    "This is useful to reduce search times. \n",
    "\n",
    "https://scikit-optimize.github.io/stable/auto_examples/parallel-optimization.html#example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from skopt import Optimizer # for the optimization\n",
    "from joblib import Parallel, delayed # for the parallelization\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8    \n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419  \\\n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9   ...     20     21      22      23      24      25      26      27   \n",
       "0  0.07871  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654  \\\n",
       "1  0.05667  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.05999  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.09744  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.05883  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       28       29  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.627417\n",
       "1    0.372583\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target:\n",
    "# percentage of benign (0) and malign tumors (1)\n",
    "\n",
    "y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset into a train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Hyperparameter Space\n",
    "\n",
    "Scikit-optimize provides an utility function to create the range of values to examine for each hyperparameters. More details in [skopt.Space](https://scikit-optimize.github.io/stable/modules/generated/skopt.Space.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the hyperparameter space\n",
    "\n",
    "param_grid = [\n",
    "    Integer(10, 120, name=\"n_estimators\"),\n",
    "    Integer(1, 5, name=\"max_depth\"),\n",
    "    Real(0.0001, 0.1, prior='log-uniform', name='learning_rate'),\n",
    "    Real(0.001, 0.999, prior='log-uniform', name=\"min_samples_split\"),\n",
    "    Categorical(['deviance', 'exponential'], name=\"loss\"),\n",
    "]\n",
    "\n",
    "# Scikit-optimize parameter grid is a list\n",
    "type(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the gradient boosting classifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function\n",
    "\n",
    "This is the hyperparameter response space, the function we want to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We design a function to maximize the accuracy, of a GBM,\n",
    "# with cross-validation\n",
    "\n",
    "# the decorator allows our objective function to receive the parameters as\n",
    "# keyword arguments. This is a requirement for scikit-optimize.\n",
    "\n",
    "@use_named_args(param_grid)\n",
    "def objective(**params):\n",
    "    \n",
    "    # model with new parameters\n",
    "    gbm.set_params(**params)\n",
    "\n",
    "    # optimization function (hyperparam response function)\n",
    "    value = np.mean(\n",
    "        cross_val_score(\n",
    "            gbm, \n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=3,\n",
    "            n_jobs=-4,\n",
    "            scoring='accuracy')\n",
    "    )\n",
    "\n",
    "    # negate because we need to minimize\n",
    "    return -value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Optimizer\n",
    "#   https://scikit-optimize.github.io/stable/modules/generated/skopt.Optimizer.html\n",
    "# In contrast to other optimization functions,\n",
    "# this one doesn't run the entire optimization when instantiated,\n",
    "# instead, we need to run optimizer.ask() on it\n",
    "# which we do in different threads using joblib.Parallel.\n",
    "# We define the number of parallel jobs with\n",
    "#   n_jobs=n_points = 4 parallel jobs\n",
    "# Additionally, the number of initial evaluations is defined here, too\n",
    "#   n_initial_points=10\n",
    "n_points = 4\n",
    "optimizer = Optimizer(\n",
    "    dimensions = param_grid, # the hyperparameter space\n",
    "    base_estimator = \"GP\", # the surrogate; options: \"GP\", \"RF\", \"ET\", \"GBRT\"\n",
    "    n_initial_points=10, # the number of points to evaluate f(x) to start of\n",
    "    acq_func='EI', # the acquisition function\n",
    "    random_state=0, \n",
    "    n_jobs=n_points,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msagardi\\AppData\\Local\\anaconda3\\envs\\hyp\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\msagardi\\AppData\\Local\\anaconda3\\envs\\hyp\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    }
   ],
   "source": [
    "# Now, we run the calls in parallel,\n",
    "# we will use 4 CPUs, i.e., 4 parallel jobs (n_points);\n",
    "# if we loop 10 times using 4 end points, we perform 40 searches in total\n",
    "n_calls = 10\n",
    "for i in range(n_calls):\n",
    "    # n_points: number of parallel jobs\n",
    "    x = optimizer.ask(n_points=n_points)  # x is a list of parallel hyperparameter spaces \n",
    "    y = Parallel(n_jobs=n_points)(delayed(objective)(v) for v in x)  # evaluate points in parallel\n",
    "    optimizer.tell(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[68, 4, 0.007381238832487747, 0.08704800719052391, 'exponential'],\n",
       " [118, 2, 0.00010113718979245275, 0.05725990689319986, 'deviance'],\n",
       " [17, 3, 0.00022221129238269847, 0.17371729808265857, 'exponential'],\n",
       " [42, 4, 0.000960176974739521, 0.6793527084375192, 'exponential'],\n",
       " [38, 5, 0.053126979002083165, 0.06277193933628669, 'deviance'],\n",
       " [28, 4, 0.005445788189169609, 0.002258808366805843, 'deviance'],\n",
       " [56, 1, 0.0006780193306440557, 0.9274466173670369, 'exponential'],\n",
       " [42, 4, 0.08952334707464486, 0.20644568894340298, 'deviance'],\n",
       " [68, 1, 0.0010637763908757617, 0.0037524397848558923, 'deviance'],\n",
       " [48, 4, 0.0016815858162959685, 0.27886812907463643, 'deviance'],\n",
       " [107, 5, 0.016812145780049137, 0.008608554188871567, 'exponential'],\n",
       " [10, 2, 0.05193389864279602, 0.2421867730253962, 'exponential'],\n",
       " [25, 3, 0.07191463533071468, 0.09221435880215857, 'deviance'],\n",
       " [120, 5, 0.07697017752330598, 0.8551443403588459, 'deviance'],\n",
       " [10, 2, 0.0784246754868581, 0.0011887833579162818, 'deviance'],\n",
       " [10, 4, 0.08121611587709748, 0.999, 'deviance'],\n",
       " [120, 1, 0.0777302892859414, 0.001, 'deviance'],\n",
       " [120, 1, 0.07607563983863629, 0.001, 'deviance'],\n",
       " [120, 1, 0.07630178973956034, 0.03496002313057386, 'deviance'],\n",
       " [114, 5, 0.0780400202695571, 0.001, 'deviance'],\n",
       " [120, 2, 0.0805909899552455, 0.999, 'deviance'],\n",
       " [120, 1, 0.08975112852805273, 0.999, 'deviance'],\n",
       " [120, 1, 0.08183657339284099, 0.999, 'deviance'],\n",
       " [120, 1, 0.018369652225184087, 0.5738087611857186, 'exponential'],\n",
       " [120, 1, 0.1, 0.999, 'deviance'],\n",
       " [120, 1, 0.1, 0.999, 'exponential'],\n",
       " [120, 5, 0.1, 0.999, 'exponential'],\n",
       " [120, 5, 0.014505257884616518, 0.16003483941555324, 'exponential'],\n",
       " [10, 1, 0.1, 0.001, 'exponential'],\n",
       " [10, 5, 0.1, 0.999, 'exponential'],\n",
       " [120, 1, 0.1, 0.999, 'exponential'],\n",
       " [120, 1, 0.1, 0.999, 'exponential'],\n",
       " [71, 1, 0.01776350591418605, 0.001, 'exponential'],\n",
       " [21, 1, 0.015029243159050375, 0.999, 'exponential'],\n",
       " [10, 1, 0.017159228824289947, 0.001, 'exponential'],\n",
       " [54, 1, 0.01185215287688623, 0.001, 'exponential'],\n",
       " [60, 5, 0.01786757467565986, 0.999, 'exponential'],\n",
       " [84, 5, 0.013808450008724295, 0.999, 'exponential'],\n",
       " [120, 2, 0.005104893773909278, 0.001, 'exponential'],\n",
       " [120, 3, 0.00419233067272598, 0.999, 'exponential']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the evaluated hyperparamters: X\n",
    "optimizer.Xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9171413381939697,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9296536796536796,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9347231715652767,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9221348826611985,\n",
       " -0.9146350725298094,\n",
       " -0.9321979190400244,\n",
       " -0.9673425989215462,\n",
       " -0.9246601351864511,\n",
       " -0.8994645705172021,\n",
       " -0.9497607655502392,\n",
       " -0.9472355130249867,\n",
       " -0.9472355130249867,\n",
       " -0.9221538695222905,\n",
       " -0.949741778689147,\n",
       " -0.9572985494038125,\n",
       " -0.9522670312143996,\n",
       " -0.9346662109820004,\n",
       " -0.9572795625427205,\n",
       " -0.9572795625427205,\n",
       " -0.9572795625427205,\n",
       " -0.9272043745727956,\n",
       " -0.9170274170274171,\n",
       " -0.9170274170274171,\n",
       " -0.9572795625427205,\n",
       " -0.9572795625427205,\n",
       " -0.9170274170274171,\n",
       " -0.886914255335308,\n",
       " -0.6256360598465861,\n",
       " -0.8994645705172021,\n",
       " -0.9195336826915774,\n",
       " -0.9195336826915774,\n",
       " -0.9196476038581302,\n",
       " -0.8919267866636288]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the accuracy: y, the selected score in the objective function\n",
    "optimizer.yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.057260</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.173717</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053127</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators  max_depth  min_samples_split  learning_rate         loss   \n",
       "0            68          4           0.007381       0.087048  exponential  \\\n",
       "1           118          2           0.000101       0.057260     deviance   \n",
       "2            17          3           0.000222       0.173717  exponential   \n",
       "3            42          4           0.000960       0.679353  exponential   \n",
       "4            38          5           0.053127       0.062772     deviance   \n",
       "\n",
       "   accuracy  \n",
       "0 -0.917141  \n",
       "1 -0.625636  \n",
       "2 -0.625636  \n",
       "3 -0.625636  \n",
       "4 -0.929654  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all together in one dataframe, so we can investigate further\n",
    "dim_names = ['n_estimators', 'max_depth', 'min_samples_split', 'learning_rate', 'loss']\n",
    "\n",
    "tmp = pd.concat([\n",
    "    pd.DataFrame(optimizer.Xi),\n",
    "    pd.Series(optimizer.yi),\n",
    "], axis=1)\n",
    "\n",
    "tmp.columns = dim_names + ['accuracy']\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate convergence of the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is equivalent to plot_convergence()\n",
    "tmp['accuracy'].sort_values(ascending=False).reset_index(drop=True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trade-off with parallelization, is that we will not optimize the search after each evaluation of f(x), instead after, in this case 4, evaluations of f(x). Thus, we may need to perform more evaluations to find the optima. But, because we do it in parallel, overall, we reduce wall time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.076970</td>\n",
       "      <td>0.855144</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.967343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.089751</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.957299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.081837</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.952267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.077730</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>0.080591</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.949742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076076</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.947236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076302</td>\n",
       "      <td>0.034960</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.947236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.089523</td>\n",
       "      <td>0.206446</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.934723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.573809</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.934666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.071915</td>\n",
       "      <td>0.092214</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.932198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053127</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014505</td>\n",
       "      <td>0.160035</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.927204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.078425</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.924660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>114</td>\n",
       "      <td>5</td>\n",
       "      <td>0.078040</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.922154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>107</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016812</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.922135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.919648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>0.017868</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.919534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>84</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013808</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.919534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.051934</td>\n",
       "      <td>0.242187</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.914635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011852</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.899465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.081216</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.899465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.891927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.886914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.278868</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.927447</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017159</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.173717</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.057260</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  min_samples_split  learning_rate         loss   \n",
       "13           120          5           0.076970       0.855144     deviance  \\\n",
       "21           120          1           0.089751       0.999000     deviance   \n",
       "24           120          1           0.100000       0.999000     deviance   \n",
       "31           120          1           0.100000       0.999000  exponential   \n",
       "25           120          1           0.100000       0.999000  exponential   \n",
       "30           120          1           0.100000       0.999000  exponential   \n",
       "26           120          5           0.100000       0.999000  exponential   \n",
       "22           120          1           0.081837       0.999000     deviance   \n",
       "16           120          1           0.077730       0.001000     deviance   \n",
       "20           120          2           0.080591       0.999000     deviance   \n",
       "17           120          1           0.076076       0.001000     deviance   \n",
       "18           120          1           0.076302       0.034960     deviance   \n",
       "7             42          4           0.089523       0.206446     deviance   \n",
       "23           120          1           0.018370       0.573809  exponential   \n",
       "12            25          3           0.071915       0.092214     deviance   \n",
       "4             38          5           0.053127       0.062772     deviance   \n",
       "27           120          5           0.014505       0.160035  exponential   \n",
       "14            10          2           0.078425       0.001189     deviance   \n",
       "19           114          5           0.078040       0.001000     deviance   \n",
       "10           107          5           0.016812       0.008609  exponential   \n",
       "38           120          2           0.005105       0.001000  exponential   \n",
       "36            60          5           0.017868       0.999000  exponential   \n",
       "37            84          5           0.013808       0.999000  exponential   \n",
       "0             68          4           0.007381       0.087048  exponential   \n",
       "28            10          1           0.100000       0.001000  exponential   \n",
       "29            10          5           0.100000       0.999000  exponential   \n",
       "32            71          1           0.017764       0.001000  exponential   \n",
       "11            10          2           0.051934       0.242187  exponential   \n",
       "35            54          1           0.011852       0.001000  exponential   \n",
       "15            10          4           0.081216       0.999000     deviance   \n",
       "39           120          3           0.004192       0.999000  exponential   \n",
       "33            21          1           0.015029       0.999000  exponential   \n",
       "9             48          4           0.001682       0.278868     deviance   \n",
       "8             68          1           0.001064       0.003752     deviance   \n",
       "6             56          1           0.000678       0.927447  exponential   \n",
       "5             28          4           0.005446       0.002259     deviance   \n",
       "3             42          4           0.000960       0.679353  exponential   \n",
       "34            10          1           0.017159       0.001000  exponential   \n",
       "2             17          3           0.000222       0.173717  exponential   \n",
       "1            118          2           0.000101       0.057260     deviance   \n",
       "\n",
       "    accuracy  \n",
       "13 -0.967343  \n",
       "21 -0.957299  \n",
       "24 -0.957280  \n",
       "31 -0.957280  \n",
       "25 -0.957280  \n",
       "30 -0.957280  \n",
       "26 -0.957280  \n",
       "22 -0.952267  \n",
       "16 -0.949761  \n",
       "20 -0.949742  \n",
       "17 -0.947236  \n",
       "18 -0.947236  \n",
       "7  -0.934723  \n",
       "23 -0.934666  \n",
       "12 -0.932198  \n",
       "4  -0.929654  \n",
       "27 -0.927204  \n",
       "14 -0.924660  \n",
       "19 -0.922154  \n",
       "10 -0.922135  \n",
       "38 -0.919648  \n",
       "36 -0.919534  \n",
       "37 -0.919534  \n",
       "0  -0.917141  \n",
       "28 -0.917027  \n",
       "29 -0.917027  \n",
       "32 -0.917027  \n",
       "11 -0.914635  \n",
       "35 -0.899465  \n",
       "15 -0.899465  \n",
       "39 -0.891927  \n",
       "33 -0.886914  \n",
       "9  -0.625636  \n",
       "8  -0.625636  \n",
       "6  -0.625636  \n",
       "5  -0.625636  \n",
       "3  -0.625636  \n",
       "34 -0.625636  \n",
       "2  -0.625636  \n",
       "1  -0.625636  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.sort_values(by='accuracy', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
